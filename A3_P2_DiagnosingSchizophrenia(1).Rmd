---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.

### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.

```{r}
#Setting working directory
setwd("C:/Users/Ejer/Desktop/3. semester/Experimental Methods 3/Assignment2")

#Loading packages
library(pacman)
p_load(modelr,lmerTest,ggplot2,dplyr,MuMIn,stringr,tidyverse,plyr,Metrics,groupdata2,crqa,readr,raster,caret,pROC,optimx,boot)

#Reading the data
schizo_df = read.csv("AcousticFeatures.csv")

schizo_df = subset(schizo_df, select = -c(X))

schizo_df$ID = as.factor(schizo_df$ID)

schizo_df$study = as.factor(schizo_df$study)

schizo_df$diagnosis = as.factor(schizo_df$diagnosis)

schizo_df$diagnosis = plyr::revalue(schizo_df$diagnosis,c("0"="control","1"="schizophrenia"))

m1 = glmer(diagnosis~1+scale(rangev)+scale(trial)+(1+scale(trial)|study),schizo_df,family = binomial)
summary(m1)

#Range significantly predict diagnosis

```

Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!

```{r}
schizo_df$PredictionsLogit=predict(m1)
schizo_df$PredictionsPerc=inv.logit(schizo_df$PredictionsLogit)
range(schizo_df$PredictionsPerc)

#Getting confusion matirix (has accuracy, kappa, sensitivity, specificity etc.)
schizo_df$Predictions[schizo_df$PredictionsPerc>0.5]="schizophrenia"
schizo_df$Predictions[schizo_df$PredictionsPerc<=0.5]="control"
conf = confusionMatrix(data = schizo_df$Predictions, reference = schizo_df$diagnosis, positive = "schizophrenia") 
conf

#ROC curve
rocCurve = roc(response = schizo_df$diagnosis, predictor = schizo_df$PredictionsPerc)
auc(rocCurve) #area under the curve --> should be reported
ci(rocCurve)
plot(rocCurve, legacy.axes = TRUE) 
```

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.

```{r}
getPerformance = function(test_df,train_df, model){
  #Getting performance from test dataframe
  test_df$PredictionsPerc = inv.logit(predict(model, test_df,allow.new.levels = T))
  test_df$Predictions[test_df$PredictionsPerc>0.5]="schizophrenia"
  test_df$Predictions[test_df$PredictionsPerc<=0.5]="control"
  conf_test = confusionMatrix(data = test_df$Predictions, reference = test_df$diagnosis, positive = "schizophrenia")
  rocCurve_test = roc(response = test_df$diagnosis, predictor = test_df$PredictionsPerc)
  
  #Get performance from training dataframe
  train_df$PredictionsPerc = inv.logit(predict(model, train_df, allow.new.levels = T))
  train_df$Predictions[train_df$PredictionsPerc>0.5]="schizophrenia"
  train_df$Predictions[train_df$PredictionsPerc<=0.5]="control"
  conf_train = confusionMatrix(data = train_df$Predictions, reference = train_df$diagnosis, positive = "schizophrenia")
  rocCurve_train = roc(response = train_df$diagnosis, predictor = train_df$PredictionsPerc)
 
   result_df = data.frame(Acc_test = conf_test$overall[1], 
                          Sensitivity_test = conf_test$byClass[1], 
                          Specificity_test = conf_test$byClass[2], 
                          PPV_test = conf_test$byClass[3], 
                          NPV_test = conf_test$byClass[4],
                          auc_test = auc(rocCurve_test),
          
                          Acc_train = conf_train$overall[1], 
                          Sensitivity_train = conf_train$byClass[1], 
                          Specificity_train = conf_train$byClass[2], 
                          PPV_train = conf_train$byClass[3], 
                          NPV_train = conf_train$byClass[4],
                          auc_train = auc(rocCurve_train))
  
  return(result_df) 
}



#temp info (for testing the CrossVal function)
#data = schizo_df
#numfolds = 4
#id_col = "ID"
#modelString = "diagnosis~1+rangev+trial+(1+trial|ID) +(1|study)"
#fold = 1
#test_df = temp_test
#train_df = temp_train
#model = temp_model

CrossVal = function(data, numfolds, id_col = NULL, cat_col = NULL, modelString){
temp = fold(data, numfolds ,id_col=id_col, cat_col=cat_col)

for (fold in seq(numfolds)){
  temp_train = subset(temp, .folds !=fold)
  temp_test = subset(temp, .folds == fold)
  temp_model=glmer(modelString,temp_train,family = "binomial",control = glmerControl(optimizer = "nloptwrap", calc.derivs = FALSE)) #training the model 
  
  temp_df = getPerformance(temp_test,temp_train, temp_model)
 temp_df$mdl_string = modelString
 temp_df$fold_nr = numfolds
  if (fold == 1){
  perf_df = temp_df
  } else {
  perf_df = rbind(perf_df, temp_df)
  }
  }
return(perf_df)
}

perf_df = CrossVal(schizo_df, numfolds = 4, id_col = "ID", cat_col = c("diagnosis"), modelString = "diagnosis~1+rangev+trial+(1+trial|study)")

mean(perf_df$auc_test)

```
N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?

### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?

```{r}
predictors = colnames(schizo_df[! colnames(schizo_df) %in% c("ID","diagnosis","trial","study","Predictions","PredictionsPerc", "PredictionsLogit", "delay", "radius", "embed")])

predictors

for (predictor in predictors){
  model_string = paste("diagnosis~1+trial+",predictor,"+(1+trial|study)", sep ="")
  temp = CrossVal(data = schizo_df, numfolds = 4, id_col = "ID", cat_col = c("diagnosis"), modelString = model_string)
  temp$predictor = predictor
  if (predictor == predictors[1]){
    perf_df1 = temp
  } else {
    perf_df1 = rbind(perf_df1, temp)
  }
}

perf_df2 = group_by(perf_df1, predictor) %>% 
  dplyr::summarise(mean_acc_test=mean(Acc_test, na.rm = T),
                   mean_sensitivity_test=mean(Sensitivity_test, na.rm = T),
                   mean_specificity_test = mean(Specificity_test, na.rm = T),
                   mean_PPV_test = mean(PPV_test, na.rm = T),
                   mean_NPV_test = mean(NPV_test, na.rm = T),
                   mean_auc_test = mean(auc_test, na.rm = T),
                   mean_acc_train=mean(Acc_train, na.rm = T),
                   mean_sensitivity_train=mean(Sensitivity_train, na.rm = T),
                   mean_specificity_train = mean(Specificity_train, na.rm = T),
                   mean_PPV_train = mean(PPV_train, na.rm = T),
                   mean_NPV_train = mean(NPV_train, na.rm = T),
                   mean_auc_train = mean(auc_train, na.rm = T))

#Use perf_df2 to see which predictor is best predictor - look at mean AUC for test!
```

### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?

Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.

```{r}
#making a function, which can perform multiple crossVal - number of crossVal er specified using rep
multipleCrossVal = function(data, modelString, numfolds, id_col = NULL, cat_col = NULL, rep){
  for (i in seq(rep)){ #creating a loop with goes through the sequence of the rep
    temp = CrossVal(data, numfolds, id_col = id_col, cat_col = cat_col, modelString = modelString) #creates a temp df with crossVal values
    temp$rep = i #creating a rep column where value is number of rep
    temp$numrep = rep
    if (i == seq(rep)[1]){result_df = temp #for first rep, the function fills in the values in temp df into the result df
    } else {result_df = rbind(result_df,temp)} #for all other rep - binding the new dataframes (so it doesn't overwrite)
  }
  return(result_df)
}

#making an empty list of modelstrings to test
modelStrings = c()

#Extracting the various predictors an saving them in an object
predictors = colnames(schizo_df)[! colnames(schizo_df)%in% c("diagnosis", "study", "trial", "ID", "Predictions","PredictionsPerc", "PredictionsLogit", "delay", "radius", "embed")]

predictors

#making a loop which goes through the extracted predictors, paste them into a model string and save these model string til the list model_strings:
for (predictor in predictors){
  modelString = paste("diagnosis~1+trial+",predictor," + (1+trial|study)", sep ="")
  modelStrings = c(modelStrings,modelString) #Now we have a list of awesome modelstring
}

modelStrings

#Adding interactions between acoustic features and trial to modelstrings
for (predictor in predictors){
  modelString = paste("diagnosis~1+trial*",predictor," + (1+trial|study)", sep ="")
  modelStrings = c(modelStrings,modelString)}

#Creating a loop, applying the multipleCrossVal function to all the modelstrings in the list modelStrings
for (modelString in modelStrings){
  temp = multipleCrossVal(data = schizo_df, modelString = modelString, numfolds = 4, id_col = "ID", cat_col = c("diagnosis"), rep = 3)
  if (modelString == modelStrings[1]){
    perf_df4 = temp 
  } else {
    perf_df4 = rbind(perf_df4,temp)
  }
}


#Getting the average performance for all the models into a dateframe - remember: it's test results that count, not training  

perf_df5 = group_by(perf_df4, mdl_string) %>%
  dplyr::summarise(mean_acc_test=mean(Acc_test, na.rm = T),
                   mean_sensitivity_test=mean(Sensitivity_test, na.rm = T),
                   mean_specificity_test = mean(Specificity_test, na.rm = T),
                   mean_PPV_test = mean(PPV_test, na.rm = T),
                   mean_NPV_test = mean(NPV_test, na.rm = T),
                   mean_auc_test = mean(auc_test, na.rm = T),
                   mean_acc_train=mean(Acc_train, na.rm = T),
                   mean_sensitivity_train=mean(Sensitivity_train, na.rm = T),
                   mean_specificity_train = mean(Specificity_train, na.rm = T),
                   mean_PPV_train = mean(PPV_train, na.rm = T),
                   mean_NPV_train = mean(NPV_train, na.rm = T),
                   mean_auc_train = mean(auc_train, na.rm = T))

```

Remember:
- Out-of-sample error crucial to build the best model!
- After choosing the model, send Celine and Riccardo the code of your model

### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data? That is, how did you extract the data, designed the models and compared their performance?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.

### Bonus question 5

You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?

### Bonus question 6

Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.
